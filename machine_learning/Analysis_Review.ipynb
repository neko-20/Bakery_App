{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec, KeyedVectors, TfidfModel\n",
    "import MeCab\n",
    "import neologdn\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from gensim import corpora\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('') # NEologd辞書指定\n",
    "\n",
    "tagger.parse('')\n",
    "def tokenize_ja(text, lower):\n",
    "    node = tagger.parseToNode(str(text))\n",
    "    while node:\n",
    "        if lower and node.feature.split(',')[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n",
    "            yield node.surface.lower()\n",
    "        node = node.next\n",
    "def tokenize(content, token_min_len, token_max_len, lower):\n",
    "    return [\n",
    "        str(token) for token in tokenize_ja(content, lower)\n",
    "        if token_min_len <= len(token) <= token_max_len and not token.startswith('_')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの読み込み\n",
    "df = pd.read_csv('') # レビュー格納csvを指定\n",
    "df_bakery = df.groupby(['store_name','score','review_cnt'])['review'].apply(list).apply(' '.join).reset_index().sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要な単語を除く\n",
    "def remove_unnecessary(text):\n",
    "    # 半角と全角の統一と重ね表現の除去\n",
    "    normalized_text = neologdn.normalize(text)\n",
    "    # URLの除去\n",
    "    text_without_url = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', normalized_text)\n",
    "    # 絵文字の除去\n",
    "    text_without_emoji = ''.join(['' if c in emoji.UNICODE_EMOJI else c for c in text_without_url])\n",
    "    # 桁区切りの除去と数字の置換\n",
    "    tmp = re.sub(r'(\\d)([,.])(\\d+)', r'\\1\\3', text_without_emoji)\n",
    "    text_replaced_number = re.sub(r'\\d+', '0', tmp)\n",
    "    # 半角記号の置換\n",
    "    tmp = re.sub(r'[!-/:-@[-`{-~]', r' ', text_replaced_number)\n",
    "    # 全角記号の置換 (ここでは0x25A0 - 0x266Fのブロックのみを除去)\n",
    "    text_removed_symbol = re.sub(u'[■-♯]', ' ', tmp)\n",
    "    return(text_removed_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパス作成\n",
    "wakati_bakery_text = []\n",
    "for i in df_bakery['review']:\n",
    "    text = remove_unnecessary(i)\n",
    "    wakati = tokenize(text, 2, 10000, True)\n",
    "    wakati_bakery_text.append(wakati)\n",
    "\n",
    "wakati_bakery_list = np.asarray(wakati_bakery_text, dtype = object) \n",
    "np.savetxt(\" \", wakati_bakery_list, fmt = '%s', delimiter = ',', encoding = 'utf-8') # コーパス保存ディレクトリ指定\n",
    "\n",
    "# モデル作成\n",
    "word2vec_bakery_model3 = word2vec.Word2Vec(wakati_bakery_text, sg = 1, vector_size = 100, window = 5, min_count = 5, epochs = 100, workers = 3)\n",
    "\n",
    "#sg（0: CBOW, 1: skip-gram）,vector_size（ベクトルの次元数）,window（学習に使う前後の単語数）,min_count（n回未満登場する単語を破棄）,epochs（トレーニング反復回数）\n",
    "\n",
    "# モデルのセーブ\n",
    "word2vec_bakery_model3.save(\" \") # word2vecモデル保存ディレクトリ指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の頻出度をカウント\n",
    "# 辞書のキー値を変更\n",
    "def change_dict_key(d, old_key, new_key, default_value=None):\n",
    "    d[new_key] = d.pop(old_key, default_value)\n",
    "\n",
    "# ベーカリーワード作成\n",
    "def make_bakery_word():\n",
    "\n",
    "    # モデル読み込み\n",
    "    model = KeyedVectors.load(' ') # word2vecモデル保存ディレクトリ指定\n",
    "\n",
    "    # wordとvectorのリスト\n",
    "    max_vocab = 40000\n",
    "    vocab = list(model.wv.index_to_key)[:max_vocab]\n",
    "    vectors = [model.wv[word] for word in vocab]\n",
    "\n",
    "    # k-meansクラスタリング\n",
    "    n_clusters = 6 #クラスター数はこちらで任意の値を定める\n",
    "    kmeans_model = KMeans(n_clusters=n_clusters, verbose=0, random_state=42)\n",
    "    kmeans_model.fit(vectors)\n",
    "\n",
    "    # クラスタ辞書化\n",
    "    cluster_labels = kmeans_model.labels_\n",
    "    cluster_to_words = defaultdict(list)\n",
    "    for cluster_id, word in zip(cluster_labels, vocab):\n",
    "        cluster_to_words[cluster_id].append(word)\n",
    "        \n",
    "    for words in cluster_to_words.values():\n",
    "        print(words[:60])\n",
    "\n",
    "    # 辞書のキー値を変更\n",
    "    change_dict_key(cluster_to_words, 0, 'その他1')\n",
    "    change_dict_key(cluster_to_words, 1, 'パンの材料')\n",
    "    change_dict_key(cluster_to_words, 2, 'その他2')\n",
    "    change_dict_key(cluster_to_words, 3, 'その他3')\n",
    "    change_dict_key(cluster_to_words, 4, 'パンの種類')\n",
    "    change_dict_key(cluster_to_words, 5, 'その他4')\n",
    "\n",
    "    df_dict = pd.DataFrame.from_dict(cluster_to_words, orient=\"index\").T\n",
    "    print(df_dict.iloc[:,[0,1,2,3,4,5]])\n",
    "\n",
    "\n",
    "    # 抽出したい分類のみbakery_wordに入れる\n",
    "    value_words = cluster_to_words['パンの種類']\n",
    "    taste_words = cluster_to_words['パンの材料']\n",
    "    value_words.extend(taste_words)\n",
    "    bakery_word = value_words\n",
    "\n",
    "    # いらないワードを削除\n",
    "    delete_words = {'生地', '使用', '風味', 'タイプ', '風味', 'リベイク', '上品', '個人的', \n",
    "                    'ちり', '丸く', 'トロ', '熱々', '個人的', 'お水', '香り', '見た目', \n",
    "                    '食感', '大人', 'パリ', 'わり', 'パン生地', '表面', 'テクスチャ', '旨い',\n",
    "                    '寄り', 'サイズ', '好み', '相方', '絶賛', '税抜','っぽい', 'リベイク',\n",
    "                    'たっぷり', 'ぱん', '税込', '香ばしく', '味わい', '良き', 'in', '焦げ', '硬さ',\n",
    "                    'チョイス', '欲しかっ', '税別', '夕食', 'よい', '水分', '断面', '食べ応え','糖質',\n",
    "                    '心地', 'お供', '具材', 'リッチ', '次回', '控えめ', '繊細', 'get', '雑穀', '優しい',\n",
    "                    '比率', 'プラス', '田舎', 'たより', '良し', '補給', 'どっち', 'タイプ', '小さい', 'バリ',\n",
    "                    'etc', '主張', 'きめ細かく', '当日', '定番', '相性', 'それら', '共演',\n",
    "                    '翌朝', '外側', '豊か', '一口', '意外', '表現', '全体的', 'わり', '出来立て','満点',\n",
    "                    'no', 'チョイス', '控えめ', '味付け', 'モソモソ', '工夫', 'ベーシック', 'シンプル', 'cube',\n",
    "                    'アクセント', 'かつ', '懐かし', '満足感', '三角', '苦手', '特製', '好い', 'どちら', '密度',\n",
    "                    'サイズ', '提供', '両方', '季節','中身', '単品', '食後', '狙い', '限定','それぞれ', '都度', \n",
    "                    '自体', '甘酸っぱい', '〃∇〃', 'お薦め', '特徴', '少量', '空気', '小腹','めん', '形状',\n",
    "                    'フカフカ', '好物', 'リベイク', '少なめ', '歯切れ', 'っぽく', '程よく', 'ひとくち', '存在感',\n",
    "                    'ふか', '盛り', '入り', '同時', '共演', '旨い', 'プラス', 'たっぷり', 'ミニ','単品',\n",
    "                    'お水', '柔らかい', '柔らか', 'うま', 'サイズ', 'そのままで', '限定', 'インパクト', 'www',\n",
    "                    '個数', 'そのままで', '長時間', 'うまく', 'あなた', 'ならでは', 'そのもの', '選択', \n",
    "                    '人受け', '健康志向', '付き', '狙い', '珍しい', '豪華', '瀬戸内', '期待外れ', '苦味',\n",
    "                    'お供', '軽い', 'in', 'ばっち', 'sns', '入り', '見た目', 'サクッ', '漬け', 'メチャ', 'かた',\n",
    "                    '美味', 'あか', '新作', '半分', 'ずぶ', '飽き', '絡み', 'イケ', '気泡', '食欲', 'オヤツ', 'no',\n",
    "                    'わたし', '文句', '適度', '驚き', 'かた', '適当', '温かい', '満点', '独特', 'パンチ', 'オイリー',\n",
    "                    'とおり', '重視', 'ボロボロ', '0g', '中間', '絶品', 'good', '苦手', 'no', '飽き', 'コシ','薄く',\n",
    "                    '瞬間', '強かっ', '期待外れ', '系統', '注入', '組み合わせ', '惜しい', 'ぅな', '控え目', '好物',\n",
    "                    '残り', '単調', '基本', 'かな', 'best', 'much', 'ふか', 'しつこ', 'がち', '無難', '加減',\n",
    "                    '現金', '切り', 'ツレ', 'あら', 'paypay', '空洞', '讃岐', '強く', '許容範囲', '口当たり',\n",
    "                    'びき', 'セール', 'しぃ', '覚醒', 'そのままで', 'よし', '重い', '帰宅', 'お酒', 'クセ', '向き', '強い',\n",
    "                    '原材料', '庶民'}\n",
    "    \n",
    "    for word in delete_words:\n",
    "        if word in bakery_word:\n",
    "            bakery_word.remove(word)\n",
    "\n",
    "    # 必要なワードを追加\n",
    "    add_words = ['食パン']\n",
    "    bakery_word.extend(add_words)\n",
    "\n",
    "    # ベーカリーワードreturn\n",
    "    return bakery_word\n",
    "\n",
    "# 頻出トップ20作成\n",
    "def make_text_top20(review_words):\n",
    "    top20 = []\n",
    "    for words in review_words:\n",
    "        c = collections.Counter(words)\n",
    "        c = c.most_common(20)\n",
    "        top20.append([i[0] for i in c])\n",
    "    return top20\n",
    "\n",
    "# tfidfトップ20作成\n",
    "def make_tfidf_top20(review_words):\n",
    "    trainings = review_words[:]\n",
    "    \n",
    "    # 単語->id変換の辞書作成\n",
    "    dictionary = corpora.Dictionary(trainings)\n",
    "\n",
    "    # textをcorpus化\n",
    "    corpus = list(map(dictionary.doc2bow, trainings))\n",
    "\n",
    "    # tfidf modelの生成\n",
    "    test_model = TfidfModel(corpus)\n",
    "\n",
    "    # corpusへのモデル適用\n",
    "    corpus_tfidf = test_model[corpus]\n",
    "\n",
    "    # id->単語へ変換\n",
    "    tfidf = [] # id -> 単語表示に変えた文書ごとのTF-IDF\n",
    "    for doc in corpus_tfidf:\n",
    "        words = []\n",
    "        for word in doc:\n",
    "            words.append([dictionary[word[0]], word[1]])\n",
    "        tfidf.append(words)\n",
    "\n",
    "    #TF-IDF値を高い順に並び替え上位単語20個に絞る。\n",
    "    top20 = [] \n",
    "    for i in range(len(tfidf)):\n",
    "        soted = sorted(tfidf[i], key=itemgetter(1), reverse=True)\n",
    "        soted_top20 = soted[:20]\n",
    "        word_list = []\n",
    "        for k in range(len(soted_top20)):\n",
    "            word = soted_top20[k][0]\n",
    "            word_list.append(word)\n",
    "        top20.append(word_list)\n",
    "\n",
    "    return top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # ベーカリーワード作成\n",
    "    bakery_word = make_bakery_word()\n",
    "\n",
    "    # レビューのワードをbakery_wordでフィルタリング\n",
    "    review_words = []\n",
    "    with open(' ','r', encoding=\"utf-8\") as f: # コーパスファイル指定\n",
    "        for data in f:\n",
    "            word = data.replace(\"'\",'').replace('[','').replace(']','').replace(' ','').replace('\\n','').split(\",\")\n",
    "            review_words.append([i for i in word if i in bakery_word])\n",
    "\n",
    "    # 頻出トップ20作成\n",
    "    text_top20 = make_text_top20(review_words)\n",
    "\n",
    "    # tfidfトップ20作成\n",
    "    tfidf_top20 = make_tfidf_top20(review_words)\n",
    "\n",
    "    # 結果をデータフレームにしてcsvに書き出す\n",
    "    df = pd.read_csv(' ') # レビュー格納csvを指定\n",
    "    df_bakery = df.groupby(['store_name','score','review_cnt'])['review'].apply(list).apply(''.join).reset_index().sort_values('score', ascending=False)\n",
    "    df_bakery['text_top20'] = text_top20\n",
    "    df_bakery['tfidf_top20'] = tfidf_top20\n",
    "    df_bakery['id'] = ['ID-' + str(i + 1).zfill(3) for i in range(len(df_bakery.index))]\n",
    "    df_bakery_sorted_top20 = df_bakery.iloc[:,[6,0,1,2,4,5]].reset_index(drop=True)\n",
    "    df_bakery_sorted_top20.to_csv(\"out_top20.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
