{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tabelog:\n",
    "    # コンストラクタ\n",
    "    def __init__(self, base_url, test_mode=False, begin_page=1, end_page=30):\n",
    "\n",
    "        # 変数宣言\n",
    "        self.store_id = ''\n",
    "        self.store_id_num = 0\n",
    "        self.store_name = ''\n",
    "        self.score = 0\n",
    "        self.review_cnt = 0\n",
    "        self.review = ''\n",
    "        self.columns = ['store_id', 'store_name', 'score', 'review_cnt', 'review']\n",
    "        self.df = pd.DataFrame(columns=self.columns)\n",
    "        self.__regexcomp = re.compile(r'\\n|\\s') # \\nは改行、\\sは空白\n",
    "\n",
    "        page_num = begin_page # 店舗一覧ページ番号\n",
    "\n",
    "        if test_mode:\n",
    "            list_url = base_url + str(page_num) + '/?Srt=D&SrtT=rt&sort_mode=1' #食べログの点数ランキングでソートする際に必要な処理\n",
    "            self.scrape_list(list_url, test_mode)\n",
    "        else:\n",
    "            while True:\n",
    "                list_url = base_url + str(page_num) + '/?Srt=D&SrtT=rt&sort_mode=1' #食べログの点数ランキングでソートする際に必要な処理\n",
    "                if self.scrape_list(list_url, test_mode) != True:\n",
    "                    break\n",
    "\n",
    "                # INパラメータまでのページ数データを取得する\n",
    "                if page_num >= end_page:\n",
    "                    break\n",
    "                page_num += 1\n",
    "\n",
    "    def scrape_list(self, list_url, test_mode):\n",
    "        \"\"\"\n",
    "        店舗一覧ページのパーシング\n",
    "        \"\"\"\n",
    "        # 取得したいページにリクエストする\n",
    "        r = requests.get(list_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            return False\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        soup_a_list = soup.find_all('a', class_='list-rst__rst-name-target') # 店名一覧\n",
    "\n",
    "        if len(soup_a_list) == 0:\n",
    "            return False\n",
    "\n",
    "        range_list = soup_a_list[:2] if test_mode else soup_a_list\n",
    "        for soup_a in range_list:\n",
    "            item_url = soup_a.get('href') # 店の個別ページURLを取得\n",
    "            self.store_id_num += 1\n",
    "            self.scrape_item(item_url)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def scrape_item(self, item_url):\n",
    "        \"\"\"\n",
    "        個別店舗情報ページのパーシング\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        r = requests.get(item_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(f'error:not found{ item_url }')\n",
    "            return\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # 店舗名称取得\n",
    "        store_name_tag = soup.find('h2', class_='display-name')\n",
    "        store_name = store_name_tag.span.string\n",
    "        print('{}→店名：{}'.format(self.store_id_num, store_name.strip()), end='')\n",
    "        self.store_name = store_name.strip()\n",
    "\n",
    "        # パン、サンドイッチ以外の店舗は除外\n",
    "        store_head = soup.find('div', class_='rdheader-subinfo') # 店舗情報のヘッダー枠データ取得\n",
    "        store_head_list = store_head.find_all('dl')\n",
    "        store_head_list = store_head_list[1].find_all('span')\n",
    "        #print('ターゲット：', store_head_list[0].text)\n",
    "\n",
    "        if store_head_list[0].text not in {'パン'}:\n",
    "            print('パン店ではないので処理対象外')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "\n",
    "        # 評価点数取得\n",
    "        rating_score_tag = soup.find('b', class_='c-rating__val')\n",
    "        rating_score = rating_score_tag.span.string\n",
    "        print('  評価点数：{}点'.format(rating_score), end='')\n",
    "        self.score = rating_score\n",
    "\n",
    "        # 評価点数が存在しない店舗は除外\n",
    "        if rating_score == '-':\n",
    "            print('  評価がないため処理対象外')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "\n",
    "        # レビュー一覧URL取得\n",
    "        review_tag_id = soup.find('li', id=\"rdnavi-review\")\n",
    "        review_tag = review_tag_id.a.get('href')\n",
    "\n",
    "        # レビュー件数取得\n",
    "        print('  レビュー件数：{}'.format(review_tag_id.find('span', class_='rstdtl-navi__total-count').em.string), end='')\n",
    "        self.review_cnt = review_tag_id.find('span', class_='rstdtl-navi__total-count').em.string\n",
    "\n",
    "        # レビュー一覧ページ番号\n",
    "        page_num = 1 #1ページ*20 = 20レビュー 。この数字を変えて取得するレビュー数を調整。\n",
    "\n",
    "        # レビュー一覧ページから個別レビューページを読み込み、パーシング\n",
    "        # 店舗の全レビューを取得すると、食べログの評価ごとにデータ件数の濃淡が発生してしまうため、\n",
    "        # 取得するレビュー数は１ページ分としている（件数としては１ページ*20=２0レビュー）\n",
    "        while True:\n",
    "            review_url = review_tag + 'COND-0/smp1/?lc=0&rvw_part=all&PG=' + str(page_num)\n",
    "            #print('\\t口コミ一覧リンク：{}'.format(review_url))\n",
    "            print(' . ' , end='')\n",
    "            if self.scrape_review(review_url) != True:\n",
    "                break\n",
    "            if page_num >= 4:\n",
    "                break\n",
    "            page_num += 1\n",
    "\n",
    "        process_time = time.time() - start\n",
    "        print('  取得時間：{}'.format(process_time))\n",
    "\n",
    "    def scrape_review(self, review_url):\n",
    "        \"\"\"\n",
    "        レビュー一覧ページのパーシング\n",
    "        \"\"\"\n",
    "        r = requests.get(review_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(f'error:not found{ review_url }')\n",
    "            return False\n",
    "        time.sleep(3)\n",
    "\n",
    "        # 各個人の口コミページ詳細へのリンクを取得する\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        review_url_list = soup.find_all('div', class_='rvw-item') # 口コミ詳細ページURL一覧\n",
    "\n",
    "        if len(review_url_list) == 0:\n",
    "            return False\n",
    "\n",
    "        for url in review_url_list:\n",
    "            review_detail_url = 'https://tabelog.com' + url.get('data-detail-url')\n",
    "\n",
    "            # 口コミのテキストを取得\n",
    "            self.get_review_text(review_detail_url)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_review_text(self, review_detail_url):\n",
    "        \"\"\"\n",
    "        口コミ詳細ページをパーシング\n",
    "        \"\"\"\n",
    "        r = requests.get(review_detail_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(f'error:not found{ review_detail_url }')\n",
    "            return\n",
    "        time.sleep(3)\n",
    "\n",
    "        # ２回以上来訪してコメントしているユーザは最新の1件のみを採用\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        review = soup.find_all('div', class_='rvw-item__rvw-comment')#reviewが含まれているタグの中身をすべて取得\n",
    "        if len(review) == 0:\n",
    "            review = ''\n",
    "        else:\n",
    "            review = review[0].p.text.strip() # strip()は改行コードを除外する関数\n",
    "\n",
    "        self.review = review\n",
    "\n",
    "        # データフレームの生成\n",
    "        self.make_df()\n",
    "\n",
    "    def make_df(self):\n",
    "        self.store_id = str(self.store_id_num).zfill(8) #0パディング\n",
    "        df2 = pd.DataFrame([[self.store_id, self.store_name, self.score, self.review_cnt, self.review]], columns=self.columns) #行を作成\n",
    "        self.df = pd.concat([self.df, df2], axis=0, ignore_index=True) # データフレームに行を追加\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    bakery_review = Tabelog(base_url=\" \",test_mode=False) # url指定\n",
    "    #CSV保存\n",
    "    bakery_review.df.to_csv(\" \") # 保存csv指定 "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
